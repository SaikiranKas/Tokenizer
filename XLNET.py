from transformers import XLNetTokenizer

# Load pre-trained XLNet tokenizer
tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased")

# Example text to tokenize

inp=["Photosynthesis","Nonetheless","Wow! üöÄ AI is evolving fast. #MachineLearning #AIü§ñ","Check out https://example.com, it's awesome! Also, email me at test@example.org.","The company‚Äôs revenue jumped from $5M to ‚Çπ40Cr in FY‚Äô23‚Äîan astounding 800% growth!","‡§Ü‡§ú ‡§ï‡§æ ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§∏‡•Å‡§π‡§æ‡§®‡§æ ‡§π‡•à‡•§ ‡§¨‡§ö‡•ç‡§ö‡•á ‡§™‡§æ‡§∞‡•ç‡§ï ‡§Æ‡•á‡§Ç ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§","‡∞à ‡∞∞‡±ã‡∞ú‡±Å ‡∞µ‡∞æ‡∞§‡∞æ‡∞µ‡∞∞‡∞£‡∞Ç ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞ö‡∞≤‡±ç‡∞≤‡∞ó‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞™‡∞ø‡∞≤‡±ç‡∞≤‡∞≤‡±Å ‡∞™‡∞æ‡∞∞‡±ç‡∞ï‡±ç‚Äå‡∞≤‡±ã ‡∞Ü‡∞°‡±Å‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å.","‡§Ü‡§ú‡§ö‡•á ‡§π‡§µ‡§æ‡§Æ‡§æ‡§® ‡§ñ‡•Ç‡§™ ‡§õ‡§æ‡§® ‡§Ü‡§π‡•á. ‡§Æ‡•Å‡§≤‡•á ‡§¨‡§æ‡§ó‡•á‡§§ ‡§ñ‡•á‡§≥‡§§ ‡§Ü‡§π‡•á‡§§."]
for i in range(0,len(inp)):
    text=inp[i]

# Tokenize the text
    tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

# Convert token IDs back to words
    decoded_tokens = tokenizer.convert_ids_to_tokens(tokens["input_ids"].squeeze().tolist())

# Print tokenized output
    print("Tokenized Input IDs:", tokens["input_ids"])
    print("Attention Mask:", tokens["attention_mask"])
    print("Decoded Tokens:", decoded_tokens)